<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Minimal Client VTO Demo — Real Scale (PD)</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: system-ui, -apple-system, Roboto, "Segoe UI", sans-serif; margin: 12px; background:#f5f6f8; color:#111; }
    h1 { font-size:18px; margin:0 0 8px 0; }
    .card { background:white; border-radius:10px; padding:12px; box-shadow:0 6px 18px rgba(20,20,40,0.06); max-width:920px; margin:auto; }
    #container { position: relative; width: 720px; max-width:100%; margin:12px auto; }
    /* keep video aspect ratio; don't crop (contain) */
    video {
      position: absolute;
      left: 0;
      top: 0;
      width: 100%;
      height: auto;
      border-radius: 8px;
      object-fit: contain;
      background: black;
      transform: scaleX(-1);
      -webkit-transform: scaleX(-1); /* mirror for user */
    }
    /* Canvas visually fills the container; backing store sized in JS */
    canvas {
      position: absolute;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      border-radius: 8px;
      pointer-events: none;
    }
    #controls { display:flex; gap:8px; margin-top:8px; align-items:center; justify-content:center; flex-wrap:wrap; }
    button { padding:8px 12px; border-radius:6px; border:1px solid #ddd; background:white; cursor:pointer; }
    label { font-size:13px; color:#333; }
    input[type="range"] { width:160px; vertical-align:middle; }
    input[type="number"] { width:80px; padding:6px; border-radius:6px; border:1px solid #ddd; }
    #status { font-size:12px; color:#666; text-align:center; margin-top:6px; }
    .row { display:flex; gap:8px; align-items:center; justify-content:center; margin-top:8px; flex-wrap:wrap; }
    .small { font-size:12px; color:#666; }
    #fitFeedback { font-weight:700; margin-top:8px; text-align:center; font-size:15px; }
    #fitConfidence { color:#666; font-size:13px; text-align:center; margin-top:4px; }
  </style>
</head>
<body>
  <div class="card">
    <h1>Minimal Client VTO Demo — Real Scale (PD)</h1>

    <div id="container">
      <video id="video" autoplay playsinline muted></video>
      <canvas id="overlay"></canvas>
    </div>

    <div class="row" style="justify-content:space-between;">
      <div>
        <label>Enter PD (mm):</label>
        <input id="pdInput" type="number" min="40" max="80" step="0.5" placeholder="eg. 63">
        <span class="small"> (leave blank to use default 63 mm)</span>
      </div>

      <div>
        <label>Frame width (mm):</label>
        <input id="frameWidth" type="number" min="80" max="160" value="140" step="1"> <span class="small">frame real width</span>
      </div>

      <div>
        <label>Asset native width (px):</label>
        <input id="assetWidth" type="number" min="200" max="4000" value="1600" step="10"> <span class="small">image px width</span>
      </div>
    </div>

    <div id="controls">
      <button id="captureBtn">Take Photo</button>
      <button id="toggleMirror">Toggle Mirror</button>
      <button id="switchCam">Switch Camera</button>

      <label style="margin-left:8px;">Vertical offset:
        <input id="offY" type="range" min="-200" max="200" value="5" step="1">
      </label>

      <label>Horizontal offset:
        <input id="offX" type="range" min="-200" max="200" value="0" step="1">
      </label>

      <label>Vertical stretch:
        <input id="vStretch" type="range" min="0.6" max="1.6" value="1.0" step="0.01">
      </label>

      <label>Debug:
        <input id="dbg" type="checkbox" />
      </label>
    </div>

    <div id="status">Status: <span id="statustxt">Initializing…</span></div>
    <div id="fitFeedback"></div>
    <div id="fitConfidence"></div>

    <div class="small" style="text-align:center; margin-top:6px;">
      Tip: for best fit, enter your pupillary distance (PD). If unknown, use default (63 mm) or calibrate via a credit card method.
    </div>
  </div>

  <!-- MediaPipe FaceMesh via CDN -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

<script>
(async function(){
  const video = document.getElementById('video');
  const canvas = document.getElementById('overlay');
  const containerEl = document.getElementById('container');
  const ctx = canvas.getContext('2d');
  const status = document.getElementById('statustxt');
  const captureBtn = document.getElementById('captureBtn');
  const toggleMirrorBtn = document.getElementById('toggleMirror');
  const switchCamBtn = document.getElementById('switchCam');
  const pdInput = document.getElementById('pdInput');
  const frameWidthEl = document.getElementById('frameWidth');
  const assetWidthEl = document.getElementById('assetWidth');
  const offY = document.getElementById('offY');
  const offX = document.getElementById('offX');
  const vStretch = document.getElementById('vStretch');
  const dbg = document.getElementById('dbg');

  const fitEl = document.getElementById('fitFeedback');
  const confEl = document.getElementById('fitConfidence');

  // Frame image (use your webp/png link here; same-origin recommended)
  const frameImg = new Image();
  frameImg.crossOrigin = "anonymous";
  frameImg.src = "http://ls-erp:8000/assets/vto/Frame_F.png"; // ensure same-origin or CORS-enabled

  // frame metadata defaults (you can update per SKU)
  let frameMeta = {
    frame_width_mm: Number(frameWidthEl.value) || 140,
    asset_native_width_px: Number(assetWidthEl.value) || 1600,
    anchor_x_px: null,
    anchor_y_px: null
  };

  // smoothing helper
  function lerp(a,b,t){ return a + (b-a)*t; }
  const smooth = { tx:0, ty:0, rot:0, scale:1 };
  const smoothFactor = 0.22;

  // MediaPipe face mesh
  const faceMesh = new FaceMesh({
    locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
  });
  faceMesh.setOptions({
    maxNumFaces: 1,
    refineLandmarks: true,
    minDetectionConfidence: 0.6,
    minTrackingConfidence: 0.5
  });

  let latestLandmarks = null;
  let debugMode = false;
  const DEFAULT_PD_MM = 63;

  // Iris landmark groups for refined model
  const LEFT_IRIS = [468,469,470,471,472];
  const RIGHT_IRIS = [473,474,475,476,477];

  // Convert normalized landmark to CSS pixels on canvas
  function toCanvas(p){ 
    const w = canvas.clientWidth || containerEl.clientWidth;
    const h = canvas.clientHeight || containerEl.clientHeight;
    return { x: p.x * w, y: p.y * h };
  }

  // avg helper using CSS pixels
  function avgPoints(indices, lm){
    const w = canvas.clientWidth || containerEl.clientWidth;
    const h = canvas.clientHeight || containerEl.clientHeight;
    let sx=0, sy=0;
    for(const i of indices){ sx += lm[i].x * w; sy += lm[i].y * h; }
    return { x: sx/indices.length, y: sy/indices.length };
  }

  // compute pixels per mm (PD method) - simple attempt using iris landmarks
  function computePixelPerMmFromLandmarks(lm) {
    try {
      const L = avgPoints(LEFT_IRIS, lm);
      const R = avgPoints(RIGHT_IRIS, lm);
      const pixelPD = Math.hypot(R.x - L.x, R.y - L.y);
      const userPD = Number(pdInput.value) || DEFAULT_PD_MM;
      if (!pixelPD || userPD <= 0) return null;
      return pixelPD / userPD; // pixels per mm (CSS px / mm)
    } catch(e){
      return null;
    }
  }

  // ---------------- Fit feedback: circular buffer and helpers ----------------
  class CircularBuffer {
    constructor(n){ this.n=n; this.arr=[]; }
    push(x){ this.arr.push(x); if(this.arr.length>this.n) this.arr.shift(); }
    avg(){ if(!this.arr.length) return null; return this.arr.reduce((a,b)=>a+b,0)/this.arr.length; }
    length(){ return this.arr.length; }
  }
  const ppmBuffer = new CircularBuffer(6);
  const facePxBuffer = new CircularBuffer(6);
  const fitRatioBuffer = new CircularBuffer(6);

  function computePPM_smart(lm) {
    // iris-based (preferred)
    try {
      const Liris = avgPoints(LEFT_IRIS, lm);
      const Riris = avgPoints(RIGHT_IRIS, lm);
      const pixelPD = Math.hypot(Riris.x - Liris.x, Riris.y - Liris.y);
      const userPDmm = Number(pdInput.value) || null;
      if (pixelPD && userPDmm && userPDmm>0) return pixelPD / userPDmm;
      if (pixelPD) return pixelPD / DEFAULT_PD_MM;
    } catch(e){}
    return null;
  }

  function computeFacePxWidth(lm) {
    try {
      const LT = toCanvas(lm[127]);
      const RT = toCanvas(lm[356]);
      if (LT && RT) return Math.hypot(RT.x - LT.x, RT.y - LT.y);
    } catch(e){}
    try {
      const L = toCanvas(lm[33]);
      const R = toCanvas(lm[263]);
      return Math.hypot(R.x - L.x, R.y - L.y);
    } catch(e){}
    return null;
  }

  function updateFitFeedback(lm) {
    if (!lm) {
      fitEl.textContent = '';
      confEl.textContent = '';
      return;
    }

    const ppmNow = computePPM_smart(lm); // px per mm (may be null)
    const facePxNow = computeFacePxWidth(lm); // px

    if (ppmNow) ppmBuffer.push(ppmNow);
    if (facePxNow) facePxBuffer.push(facePxNow);

    const ppm = ppmBuffer.avg();
    const facePx = facePxBuffer.avg();

    if (!ppm || !facePx) {
      confEl.textContent = 'Calculating size… (move into view / good lighting)';
      fitEl.textContent = '';
      return;
    }

    const faceMM = facePx / ppm;
    const frameMM = frameMeta.frame_width_mm || Number(frameWidthEl.value) || 140;
    const fitRatio = frameMM / faceMM;
    fitRatioBuffer.push(fitRatio);
    const fitRatioSm = fitRatioBuffer.avg();

    // thresholds (tweak)
    const t_small = 0.92, t_slightSmall = 0.98, t_slightLarge = 1.03, t_large = 1.08;

    let msg = '', color = '#111';
    if (fitRatioSm < t_small) {
      msg = `Too Small — frame is ${(fitRatioSm*100).toFixed(0)}% of your face width`;
      color = '#d9534f';
    } else if (fitRatioSm < t_slightSmall) {
      msg = `Slightly Small — frame is ${(fitRatioSm*100).toFixed(0)}% of your face width`;
      color = '#f0ad4e';
    } else if (fitRatioSm <= t_slightLarge) {
      msg = `Fits Perfect — frame is ${(fitRatioSm*100).toFixed(0)}% of your face width`;
      color = '#5cb85c';
    } else if (fitRatioSm <= t_large) {
      msg = `Slightly Oversized — frame is ${(fitRatioSm*100).toFixed(0)}% of your face width`;
      color = '#f0ad4e';
    } else {
      msg = `Oversized — frame is ${(fitRatioSm*100).toFixed(0)}% of your face width`;
      color = '#d9534f';
    }

    const usedUserPD = Boolean(pdInput.value && Number(pdInput.value) > 0);
    const confMsg = usedUserPD ? 'High confidence (using your PD).' :
                    (ppmBuffer.arr.length >= 4) ? 'Medium confidence (estimated PD used).' :
                    'Low confidence — enter PD or calibrate for best accuracy.';

    fitEl.style.color = color;
    fitEl.textContent = msg;
    confEl.textContent = confMsg;
  }

  // main draw function which uses real-world frame width and independent vertical stretch
  function drawFrameWithRealScale(lm) {
    // ensure image loaded
    if (!frameImg || !frameImg.complete || (frameImg.naturalWidth || 0) === 0) {
      // skip drawing frame until image is loaded
      return;
    }

    // Use CSS canvas sizes for layout
    const lw = canvas.clientWidth, lh = canvas.clientHeight;

    const leftEyeIdxs  = [33, 7, 163, 144, 145, 153, 154, 155, 133];
    const rightEyeIdxs = [263,249,390,373,374,380,381,382,362];
    const noseBridgeIdxs = [6,197,195,5,4];

    const L = avgPoints(leftEyeIdxs, lm);
    const R = avgPoints(rightEyeIdxs, lm);
    const NB = avgPoints(noseBridgeIdxs, lm);

    // face width estimate (temples more stable)
    const leftTemple = toCanvas(lm[127]), rightTemple = toCanvas(lm[356]);
    let faceWidth = Math.hypot(rightTemple.x - leftTemple.x, rightTemple.y - leftTemple.y);
    if (!isFinite(faceWidth) || faceWidth < 1) {
      faceWidth = Math.hypot(R.x - L.x, R.y - L.y);
    }

    // rotation (tilt)
    const angle = Math.atan2(R.y - L.y, R.x - L.x);

    // compute pixels/mm using iris PD if available
    const ppm = computePixelPerMmFromLandmarks(lm);
    let scaleX;
    if (ppm) {
      const desiredFrameWidthPx = frameMeta.frame_width_mm * ppm;
      scaleX = desiredFrameWidthPx / (frameMeta.asset_native_width_px || frameImg.naturalWidth || frameImg.width || 1600);
      status.textContent = `Detected PD scale: ${ppm.toFixed(2)} px/mm`;
    } else {
      // fallback: scale by faceWidth mapping (heuristic)
      const fallbackScaleFactor = 1.6;
      const desiredFrameWidthPx = faceWidth * fallbackScaleFactor;
      scaleX = desiredFrameWidthPx / (frameMeta.asset_native_width_px || frameImg.naturalWidth || frameImg.width || 1600);
      status.textContent = `Using fallback scale (no iris landmarks)`;
    }

    // vertical stretch factor from UI
    const verticalStretch = Number(vStretch.value) || 1.0;

    // compute anchor: eye midpoint blended with nose bridge
    const anchorX = ((L.x + R.x) / 2) * 0.85 + NB.x * 0.15;
    const anchorY = ((L.y + R.y) / 2) * 0.85 + NB.y * 0.15;

    const ASSET_VERTICAL_OFFSET = (offY && offY.value !== '') ? Number(offY.value) : -60;
    const ASSET_HORIZONTAL_OFFSET = Number(offX.value) || -10;

    // smoothing
    smooth.tx = lerp(smooth.tx, anchorX, smoothFactor);
    smooth.ty = lerp(smooth.ty, anchorY + ASSET_VERTICAL_OFFSET, smoothFactor);
    smooth.rot = lerp(smooth.rot, angle, smoothFactor);
    // keep a single stored scale for readability, but compute both draws from it
    smooth.scale = lerp(smooth.scale, scaleX, smoothFactor);

    // debug landmarks
    if (debugMode) {
      ctx.fillStyle = 'red'; ctx.beginPath(); ctx.arc(L.x, L.y, 4,0,2*Math.PI); ctx.fill();
      ctx.fillStyle = 'red'; ctx.beginPath(); ctx.arc(R.x, R.y, 4,0,2*Math.PI); ctx.fill();
      ctx.fillStyle = 'yellow'; ctx.beginPath(); ctx.arc(NB.x, NB.y, 4,0,2*Math.PI); ctx.fill();
      const ppmLocal = computePixelPerMmFromLandmarks(lm);
      if (ppmLocal) {
        const Liris = avgPoints(LEFT_IRIS, lm);
        const Riris = avgPoints(RIGHT_IRIS, lm);
        ctx.fillStyle='white'; ctx.font='12px monospace';
        ctx.fillText(`ppm:${ppmLocal.toFixed(2)}`, 10, 18);
        ctx.fillText(`PDpx:${Math.hypot(Riris.x-Liris.x,Riris.y-Liris.y).toFixed(1)}`, 10, 34);
      }
      ctx.fillStyle='white'; ctx.font='12px monospace';
      ctx.fillText(`yaw?: angle:${currentAngleLabel()}`, 10, 50); // harmless placeholder
    }

    // ---- compute native sizes and scales ----
    const nativeAssetWidth = frameMeta.asset_native_width_px || frameImg.naturalWidth || frameImg.width || 1600;
    const nativeAssetHeight = Math.round(nativeAssetWidth * 0.6);

    const scaleY = smooth.scale * verticalStretch;

    // final on-screen sizes
    const w = nativeAssetWidth * smooth.scale;
    const h = nativeAssetHeight * scaleY;

    // compute anchor inside asset
    const anchorInAssetX = frameMeta.anchor_x_px || (nativeAssetWidth / 2);
    const anchorInAssetY = frameMeta.anchor_y_px || (nativeAssetHeight / 2);

    // draw frame (mirrored handling) — use CSS canvas width for drawX
    ctx.save();
    const drawX = (canvas.clientWidth) - (smooth.tx + ASSET_HORIZONTAL_OFFSET);
    const drawY = smooth.ty;
    ctx.translate(drawX, drawY);
    ctx.rotate(-smooth.rot);

    // drawImage with independent w,h (takes care of vertical stretch)
    ctx.drawImage(
      frameImg,
      -anchorInAssetX * smooth.scale,     // dx (CSS pixels)
      -anchorInAssetY * scaleY,           // dy
      w,                                  // dwidth (CSS pixels)
      h                                   // dheight
    );
    ctx.restore();

    // update fit-feedback after drawing (smooth & stable)
    updateFitFeedback(lm);
  }

  // helper used in debug (placeholder, safe)
  function currentAngleLabel(){ return ''; }

  // on results -> draw video + overlay
  faceMesh.onResults((results) => {
    // clear using CSS pixels
    const cssW = canvas.clientWidth, cssH = canvas.clientHeight;
    ctx.clearRect(0,0, cssW, cssH);

    ctx.save();
    ctx.scale(-1,1); // mirrored draw
    // draw using CSS pixel sizes
    ctx.drawImage(results.image, -cssW, 0, cssW, cssH);
    ctx.restore();

    if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
      latestLandmarks = results.multiFaceLandmarks[0];
      drawFrameWithRealScale(latestLandmarks);
      status.textContent = 'Face detected';
    } else {
      latestLandmarks = null;
      status.textContent = 'No face detected';
      updateFitFeedback(null);
    }
  });

  // camera start
  let mpCamera = null;
  let camList = [], currentCamIndex = 0;
  async function startCamera(deviceId = undefined) {
    if (mpCamera) mpCamera.stop();
    const constraints = {
      video: {
        width: 1280,
        height: 720,
        facingMode: deviceId ? undefined : 'user',
        deviceId: deviceId ? { exact: deviceId } : undefined
      }, audio: false
    };
    try {
      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = stream;
      const cams = await navigator.mediaDevices.enumerateDevices();
      camList = cams.filter(d => d.kind === 'videoinput');
      if (deviceId) {
        const idx = camList.findIndex(c => c.deviceId === deviceId);
        if (idx >= 0) currentCamIndex = idx;
      }
      mpCamera = new Camera(video, {
        onFrame: async () => { await faceMesh.send({ image: video }); },
        width: 1280, height: 720
      });
      mpCamera.start();
    } catch(err) {
      console.error(err);
      status.textContent = 'Camera error';
    }
  }

  // UI events
  toggleMirrorBtn.addEventListener('click', () => {
    // flip CSS transform on video only (canvas stays drawn mirrored by code)
    if (video.style.transform.includes('scaleX(-1)')) {
      video.style.transform = '';
    } else {
      video.style.transform = 'scaleX(-1)';
    }
  });

  switchCamBtn.addEventListener('click', async () => {
    const cams = await navigator.mediaDevices.enumerateDevices();
    camList = cams.filter(d => d.kind === 'videoinput');
    if (camList.length <= 1) return alert('No other camera found');
    currentCamIndex = (currentCamIndex + 1) % camList.length;
    await startCamera(camList[currentCamIndex].deviceId);
  });

  captureBtn.addEventListener('click', () => {
    const dataUrl = canvas.toDataURL('image/png');
    const w = window.open('');
    w.document.body.style.margin = 0;
    const img = new Image(); img.src = dataUrl; w.document.body.appendChild(img);
  });

  // controls update handlers
  offY.addEventListener('input', () => {}); // live-used in draw
  offX.addEventListener('input', () => {});
  vStretch.addEventListener('input', () => {});
  dbg.addEventListener('change', ()=> { debugMode = dbg.checked; });

  // update frameMeta when inputs change
  frameWidthEl.addEventListener('input', ()=> {
    frameMeta.frame_width_mm = Number(frameWidthEl.value) || frameMeta.frame_width_mm;
  });
  assetWidthEl.addEventListener('input', ()=> {
    frameMeta.asset_native_width_px = Number(assetWidthEl.value) || frameMeta.asset_native_width_px;
  });

  // fitCanvas: size container using intrinsic video aspect ratio and scale canvas backing store for DPR
  function fitCanvas() {
    const layoutWidth = containerEl.clientWidth || video.getBoundingClientRect().width || 640;
    const intrinsicVideoWidth = video.videoWidth || 0;
    const intrinsicVideoHeight = video.videoHeight || 0;
    let targetWidth, targetHeight;

    if (intrinsicVideoWidth && intrinsicVideoHeight) {
      const aspect = intrinsicVideoWidth / intrinsicVideoHeight;
      targetWidth = layoutWidth;
      targetHeight = Math.round(targetWidth / aspect);
    } else {
      targetWidth = layoutWidth;
      targetHeight = Math.round(targetWidth * 9 / 16);
    }

    // set container height to CSS height
    containerEl.style.height = targetHeight + 'px';

    // set canvas backing store (physical pixels) scaled by DPR and CSS size
    const dpr = window.devicePixelRatio || 1;
    canvas.width = Math.round(targetWidth * dpr);
    canvas.height = Math.round(targetHeight * dpr);
    canvas.style.width = targetWidth + 'px';
    canvas.style.height = targetHeight + 'px';

    // set transform so drawing uses CSS pixels coordinates
    ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
  }

  video.addEventListener('loadedmetadata', fitCanvas);
  video.addEventListener('playing', fitCanvas);
  window.addEventListener('resize', fitCanvas);

  // start
  await startCamera();

  // ensure image loaded; if asset width unknown, update assetWidth input once loaded
  frameImg.onload = () => {
    if (!frameMeta.asset_native_width_px && frameImg.width) {
      frameMeta.asset_native_width_px = frameImg.width;
      assetWidthEl.value = frameImg.width;
    }
  };

})();
</script>
</body>
</html>
